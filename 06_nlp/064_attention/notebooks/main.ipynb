{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T18:03:36.315385Z",
     "start_time": "2025-04-27T18:03:36.312054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from data.datasets.tokenized import TokenizedDataset\n",
    "\n",
    "from models.from_scratch import TransformerShakespeare"
   ],
   "id": "7f9f69e1e1d13220",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformers for language generation\n",
    "\n",
    "## The goal\n",
    "\n",
    "## Language generation: write (sound like) as Shakespeare\n",
    "\n",
    "Our goal is to write as Shakespeare, but off course this problem is very difficult so by now we are content to sound like Shakespeare ðŸ˜…\n",
    "\n",
    "## The dataset\n",
    "\n",
    "To sound as Shakespeare we have a learning source: Measure for Measure, a play from William Shakespeare written in 1603, where the author\n",
    "handle topics as morality, power and forgiveness.\n",
    "\n",
    "## The model\n",
    "\n",
    "On this project you will find two models for text generation:\n",
    "\n",
    "- A transformers model built from zero\n",
    "- A fine-tuned version of GPT2 ([OpenAI paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf))\n",
    "\n",
    "## Tech stack\n",
    "\n",
    "In order to solve the problem the next libraries are used:\n",
    "\n",
    "1. Pytorch\n",
    "2. Transformers, from hugging face\n",
    "3. GTP causal model implementations and GTP tokenizers\n",
    "\n",
    "## Project structure\n",
    "\n",
    "This project involves an e2e deep learning task, from dataset processing, to model generation, so we dived the whole problem in various subfolders:\n",
    "\n",
    "- **architecture**: Common machine learning artifacts and modules, like `SelfAttention` and other layer implementations\n",
    "- **data**: All logic for load the input datasets and convert them into vectorized pairs `(x, y)`, using a tokenizer (seen later)\n",
    "- **models**: The final architectures that learns how to generate text that sounds like Shakespeare using all **architecture** artifacts.\n",
    "- **train**: The connector between datasets and models, there we handle all logic about model training and exports."
   ],
   "id": "572098dfbe887b2b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The first thing: the dataset\n",
    "\n",
    "As you may know, one of the most important parts of machine learning solutions is off course the data.\n",
    "\n",
    "In order to sound like Shakespeare we have two main sources of information, extracted from the whole *Measure by Measure* play, its document has 40k lines.\n",
    "\n",
    "- **data/static/train.txt**: The first 32k lines of *Measure by Measure* play\n",
    "- **data/static/test.txt**: The last 8k lines of *Measure by Measure* play\n",
    "\n",
    "As you can see at `data.datasets.tokenized` we implement all the logic to perform the next pipeline:\n",
    "\n",
    "![Dataset pipeline](https://media.githubusercontent.com/media/lgemc/pytorch_training_course/refs/heads/master/static/data_pipeline.png)\n",
    "\n",
    "## Batch size per words and tokens\n",
    "\n",
    "As you may know, the tokenizer receives a max amount of words to be tokenized, its context length it is not unlimited,\n",
    "so we should tokenize by chunks in order to get it working properly.\n",
    "\n",
    "So we split the whole file into chunks of size 140 words.\n",
    "\n",
    "We also use the same amount of tokens per pair x, y (x a batch of 140 tokens, y is x shifted to left with also 140 tokens)\n",
    "\n",
    "## Synthethic dataset samples expantion\n",
    "\n",
    "The whole dataset has got 202651 words, so we only can have 2026 samples for our model, in order to\n",
    "increase this and to **has a more diverse dataset in a synthetic way** we generate some random indexes\n",
    "to extract extra sequences from the dataset, additional to the original 2026 samples.\n",
    "\n",
    "## Data tokenization\n",
    "\n",
    "In order to convert the data into vectors we use the original `GTP2Tokenizer` which has a vocabulary of 50.257 tokens."
   ],
   "id": "8d498f86af563e50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T18:04:31.412348Z",
     "start_time": "2025-04-27T18:04:28.697819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size_words = 140\n",
    "max_token_length = 140\n",
    "amount_of_train_samples = 3000\n",
    "amount_of_test_samples = 600\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "train_dataset  = TokenizedDataset(\n",
    "    \"../data/static/train.txt\",\n",
    "    tokenizer = tokenizer,\n",
    "    batch_size_words = 140,\n",
    "    max_token_length = 140,\n",
    "    amount_of_samples = amount_of_train_samples,\n",
    ")\n",
    "\n",
    "test_dataset = TokenizedDataset(\n",
    "    \"../data/static/test.txt\",\n",
    "    tokenizer = tokenizer,\n",
    "    batch_size_words = 140,\n",
    "    max_token_length = 140,\n",
    "    amount_of_samples = amount_of_test_samples,\n",
    ")\n",
    "\n",
    "print(f\"Tran and test datasets loaded. Train len: {len(train_dataset)}, Test len {len(test_dataset)}\")"
   ],
   "id": "e5e9a0f112d26323",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tran and test datasets loaded. Train len: 3000, Test len 600\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## First use case: Lets train the model from zero\n",
    "\n",
    "Text generation, so how?\n",
    "\n",
    "We have a lot of ways to train a model for text generation, like `lstms`, `rnns`, `transformers`, and this time we are going to use `transformers`\n",
    "\n",
    "Why transformers you may think, lets talk about some context:\n",
    "\n",
    "### LSTMS and RNNS, strengths and weaknesses\n",
    "\n",
    "As mentioned at [A Clockwork RNN](https://arxiv.org/pdf/1402.3511), Jan Koutnin et Al, RNNS have shown high accuracy on text based tasks, as mentioned in the paper on classification tasks and sequence generation tasks.\n",
    "\n",
    "But this kind of networks has an important weakness: the required amount of time and computation power required for train this kind of networks.\n",
    "\n",
    "Why? because the architecture is based on sequential gradient calculation to be transferred from layer to layer, so one gradient at time step ti can not\n",
    "be calculated without the gradient at ti-i, so when we have high amounts of data, like wikipedia's whole dataset, resources needed exceeds the amount\n",
    "of computation power available.\n",
    "\n",
    "## Transformers: strengths and weaknesses\n",
    "\n",
    "As shown in the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) from Ashish Vaswani et al, transformer based models\n",
    "for text tasks like text translation or generation can reach an accuracy near the state of the art at the time it was released (the accuracy is almost the same\n",
    "or a little worse, +-0.2) but the training cost is one or two orders of magnitude less than those based on RNNS or convolutional neural network ones, like [Deep Recurrent Models with Fast-Forward Connections for Neural Machine\n",
    "Translation](https://arxiv.org/pdf/1606.04199).\n",
    "\n",
    "So off course transformers are an important topic, with strengths in accuracy but more importantly it reduces times and costs to train neural network models."
   ],
   "id": "26dc0d81d35bfa0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Our first model to sound as Shakespeare: lets build from scratch\n",
    "\n",
    "Our first model is a hand-made architecture based on transformers architecture proposed at **Attention is all you need** paper"
   ],
   "id": "b397be8c2a9f02ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T18:03:42.458877Z",
     "start_time": "2025-04-27T18:03:42.179762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ],
   "id": "e97027422686f65c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T18:05:24.033089Z",
     "start_time": "2025-04-27T18:05:23.965902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = max_token_length # it should match with the token length\n",
    "model_dim = 64\n",
    "heads_num = 8\n",
    "blocks_num = 8\n",
    "dropout = .4\n",
    "\n",
    "model_from_scratch = TransformerShakespeare(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    model_dim=model_dim,\n",
    "    heads_num=heads_num,\n",
    "    blocks_num=blocks_num,\n",
    "    device=device,\n",
    "    dropout=dropout,\n",
    ")\n",
    "model_from_scratch"
   ],
   "id": "a21f3fe1ecf36c2d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerShakespeare(\n",
       "  (E): Embedding(50257, 64)\n",
       "  (posE): Embedding(140, 64)\n",
       "  (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SelfAttention(\n",
       "            (Wq): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wk): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wv): Linear(in_features=64, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dense): Linear(in_features=64, out_features=64, bias=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffd): FeedForward(\n",
       "        (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop1): Dropout(p=0.1, inplace=False)\n",
       "      (drop2): Dropout(p=0.1, inplace=False)\n",
       "      (drop3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SelfAttention(\n",
       "            (Wq): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wk): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wv): Linear(in_features=64, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dense): Linear(in_features=64, out_features=64, bias=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffd): FeedForward(\n",
       "        (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop1): Dropout(p=0.1, inplace=False)\n",
       "      (drop2): Dropout(p=0.1, inplace=False)\n",
       "      (drop3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SelfAttention(\n",
       "            (Wq): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wk): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wv): Linear(in_features=64, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dense): Linear(in_features=64, out_features=64, bias=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffd): FeedForward(\n",
       "        (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop1): Dropout(p=0.1, inplace=False)\n",
       "      (drop2): Dropout(p=0.1, inplace=False)\n",
       "      (drop3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SelfAttention(\n",
       "            (Wq): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wk): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wv): Linear(in_features=64, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dense): Linear(in_features=64, out_features=64, bias=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffd): FeedForward(\n",
       "        (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop1): Dropout(p=0.1, inplace=False)\n",
       "      (drop2): Dropout(p=0.1, inplace=False)\n",
       "      (drop3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SelfAttention(\n",
       "            (Wq): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wk): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wv): Linear(in_features=64, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dense): Linear(in_features=64, out_features=64, bias=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffd): FeedForward(\n",
       "        (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop1): Dropout(p=0.1, inplace=False)\n",
       "      (drop2): Dropout(p=0.1, inplace=False)\n",
       "      (drop3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SelfAttention(\n",
       "            (Wq): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wk): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wv): Linear(in_features=64, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dense): Linear(in_features=64, out_features=64, bias=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffd): FeedForward(\n",
       "        (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop1): Dropout(p=0.1, inplace=False)\n",
       "      (drop2): Dropout(p=0.1, inplace=False)\n",
       "      (drop3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SelfAttention(\n",
       "            (Wq): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wk): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wv): Linear(in_features=64, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dense): Linear(in_features=64, out_features=64, bias=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffd): FeedForward(\n",
       "        (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop1): Dropout(p=0.1, inplace=False)\n",
       "      (drop2): Dropout(p=0.1, inplace=False)\n",
       "      (drop3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SelfAttention(\n",
       "            (Wq): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wk): Linear(in_features=64, out_features=8, bias=False)\n",
       "            (Wv): Linear(in_features=64, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dense): Linear(in_features=64, out_features=64, bias=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffd): FeedForward(\n",
       "        (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop1): Dropout(p=0.1, inplace=False)\n",
       "      (drop2): Dropout(p=0.1, inplace=False)\n",
       "      (drop3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=64, out_features=50257, bias=False)\n",
       "  (drop1): Dropout(p=0.4, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from train.from_scratch import train as train_from_scratch\n",
    "\n",
    "model_from_scratch, train_losses, test_losses =  train_from_scratch(\n",
    "    model_from_scratch,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=test_dataset,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    device=device,\n",
    "    logging_steps=4,\n",
    "    vocab_size=train_dataset.tokenizer.vocab_size\n",
    ")\n"
   ],
   "id": "3da583e2be38e59f",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
